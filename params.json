{"name":"Gridhowto","tagline":"Grid Ansible Slurm Centos","body":"Grid Howto\r\n==========\r\nThis howto is for CentOS based clusters. You can try the setup in VirtualBox as well, although you will lack BMC and IB features.\r\n\r\nThe following terminology is used: *client* is a remote or virtual machine you want ot provision, *host* is your machine (laptop) from which you provision and control the clients.\r\n\r\nIn the first step *root servers* are installed. Later on, root servers are used for large-scale cluster installation. We will use Space Jockey to provision root servers. Space Jockey is a very simple bootp tool, it does not compare with Cobbler or Xcat. Its main purpose is to boot and install root servers from your laptop. For this primordial installation you need OS X, nginx and dnsmasq.\r\n\r\nDownload the gridhowto:\r\n\r\n    cd; git clone git://github.com/hornos/gridhowto.git\r\n\r\nThe following network topology is recommended. The BMC network can be on the same interface as system (eth0). The system network is used to boot and provision the cluster.\r\n\r\n    IF   Network  Address Range\r\n    bmc  bmc      10.0.0.0/16\r\n    eth0 system   10.1.0.0/16\r\n    eth1 storage  10.2.0.0/16\r\n    eth2 mpi      10.3.0.0/16\r\n    ethX external ?\r\n\r\nThe network configuration is found in `network.yml`. Each network interface can be a bond. On high-performance systems storage and mpi is InfiniBand or other high-speed network. If you have less than 4 interfaces use alias networks. Separate external network form the others.\r\n\r\n### Root Servers in VirtualBox \r\nYou can make a virtual infrastructure in VirtualBox. Create the following virtual networks:\r\n\r\n    Network   VBox Net IPv4 Addr  Mask DHCP\r\n    system    vboxnetN 10.1.1.254 16   off\r\n    storage   vboxnetM 10.2.1.254 16   off\r\n    mpi       intnet\r\n    external  NAT/Bridged\r\n\r\nSetup the virtual server to have 2TB of disk and 4 network cards as well as network boot enabled.\r\n\r\n## Primordial Installation\r\nInstall boot servers on the host:\r\n\r\n    brew install dnsmasq nginx\r\n\r\nFrom now on all commands are relative to the `space` directory:\r\n\r\n    cd $HOME/gridhowto/space\r\n\r\nIf you don't know which machine to boot you can check bootp requests from the root servers:\r\n\r\n    ./jockey dump <INTERFACE>\r\n\r\nwhere the last argument is the interface to listen on eg. vboxnet0.\r\n\r\nThe recommended way insert an installation DVD in each server and leave the disk in the drive. You can consider it as a rescue system.\r\n\r\nCreate the `boot/centos64` directory and put `vmlinuz` and `initrd.img` from the CentOS install media (`isolinux` directory). Edit the `kickstart.centos64` file if you want to customize the installatio (especially `NETWORK` and `HOSTNAME` section). Put `pxelinux.0, chain.c32` from the syslinux 4.X package into `boot`.\r\n\r\nSet the address of the host machine (your laptop's corresponding interface). In this example \r\n\r\n    ./jockey host 10.1.1.254\r\n\r\nor you can give an interface and let the script autodetect the host IP:\r\n\r\n    ./jockey @host vboxnet5\r\n\r\nKickstart a MAC address with the CentOS installation:\r\n\r\n    ./jockey centos64 08:00:27:14:68:75\r\n\r\nYou can use `-` instead of `:`. Letters are converted to lowercase.\r\n\r\nThe `centos64` command creates a kickstart file in `boot` and a pxelinux configuration in `boot/pxelinux.cfg`. It also generates a root password which you can use for the stage 2 provisioning. Edit kickstarts (`boot/*.ks` files) after kicked. Root passwords are in `*.pass` files. After you secured the install root user is not allowed to login remotely.\r\n\r\nFinish the preparatin by starting the boot servers (http, dnsmasq) each in a separate terminal:\r\n\r\n    ./jockey http\r\n    ./jockey boot\r\n\r\nBoot servers listen on the IP you specified by the `host` command. The boot process should start now and the automatic installation continues. If finished change the boot order of the machine by:\r\n\r\n    ./jockey local 08:00:27:14:68:75\r\n\r\nThis command changes the pxelinux order to local boot. You can also switch to local boot by IPMI for real servers.\r\n\r\n### Install from URL\r\nMount install media and link under `boot/centos64/repo`. Edit the kickstart file and change `cdrom` to:\r\n\r\n    url --url http://10.1.1.254:8080/centos64/repo\r\n\r\nWhere the URL is the address of the nginx server running on the host.\r\n\r\n### VNC-based Graphical Install\r\nFor headless installation use VNC. Edit the corresponding file in `boot/pxelinux.cfg` and set the following kernel parameters:\r\n\r\n    APPEND vnc ...\r\n\r\nVNC is started without password. Connect your VNC client to eg. `10.1.1.1:1`.\r\n\r\n### Hardware Detection\r\nFor hardware detection you need to have the following files installed from syslinux:\r\n\r\n    boot/hdt.c32\r\n\r\nSwitch to detection (and reboot the machine):\r\n\r\n    ./jockey detect 08:00:27:14:68:75 \r\n\r\n### Firmware Upgrade with FreeDOS\r\nThis section is based on http://wiki.gentoo.org/wiki/BIOS_Update . You have to use a Linux host to create the bootdisk image. You have to download freedos tools from ibiblio.org:\r\n\r\n    dd if=/dev/zero of=freedos bs=1024 count=20480\r\n    mkfs.msdos freedos\r\n    unzip sys-freedos-linux.zip && ./sys-freedos.pl --disk=freedos\r\n    mkdir $PWD/mnt; mount -o loop freedos /mnt\r\n\r\nCopy the firmware upgrade files to `$PWD/mnt` and umount the disk. Put `memdisk` and `freedos` to `boot` directory and switch to firmware (and reboot the machine):\r\n\r\n    ./jockey firmware 08:00:27:14:68:75\r\n\r\n### Install ESXi 5.X\r\nYou have to use syslinux 4.X . Mount ESXi install media under `boot/esxi/repo`. Put `mboot.c32` from the install media into jockey's root directory. Kickstart the machine to boot ESXi installer:\r\n\r\n    ./jockey esxi 08:00:27:14:68:75\r\n\r\nEdit the kickstart file if you want to change the default settings.\r\n\r\n### Other Mini-Linux Variants\r\nYou can boot Cirros and Tiny Linux as well. For CirrOS put `initrd.img` and `vmlinuz` into `boot/cirros`, for Tiny Linux put `core.gz` and `vmlinuz` into `boot/tiny`, and switch eg. to Tiny:\r\n\r\n    ./jockey tiny 08:00:27:14:68:75\r\n\r\n### Kickstart from scratch\r\nA good starting point for a kickstart can be found in the EAL4 package:\r\n\r\n    cd src\r\n    wget ftp://ftp.pbone.net/mirror/ftp.redhat.com/pub/redhat/linux/eal/EAL4_RHEL5/DELL/RPMS/lspp-eal4-config-dell-1.0-1.el5.noarch.rpm\r\n    rpm2cpio lspp-eal4-config-dell-1.0-1.el5.noarch.rpm | cpio -idmv\r\n\r\n## IPMI Basics\r\nIf you happen to have real metal servers you need to deal with IPMI as well. Enterprise class machiens contain a small computer which you can use to remote control the machine. IPMI interfaces connect to the bmc network. Install ipmitools:\r\n\r\n    brew install ipmitool\r\n\r\nYou can register IPMI users with different access levels. Connect to the remote machine with the default settings:\r\n\r\n    ipmitool -I lanplus -U admin -P admin -H <BMC IP>\r\n\r\nGet a remote remote console:\r\n\r\n    xterm -e \"ipmitool -I lanplus -U admin -P admin -H <BMC IP> sol activate\"\r\n\r\nGet sensor listing:\r\n\r\n    ipmitool -I lanplus -U admin -P admin -H <BMC IP> sdr\r\n\r\n### IPMI Management with Space Jockey\r\nSetup IPMI adresses according to the network topology. Dip OS X into the IPMI LAN:\r\n\r\n    sudo ifconfig en0 alias 10.0.1.254 255.255.0.0\r\n\r\nSet the IPMI user and password:\r\n\r\n    ./jockey ipmi user admin admin\r\n\r\nGet a serial-over-lan console:\r\n\r\n    ./jockey ipmi tool 10.0.1.1 sol active\r\n\r\nGet the power status:\r\n\r\n    ./jockey ipmi tool 10.0.1.1 chassis status\r\nReboot a machine:\r\n\r\n    ./jockey ipmi tool 10.0.1.1 power reset\r\n\r\nForce PXE boot on the next boot only:\r\n\r\n    ./jockey ipmi tool 10.0.1.1 chassis bootdev pxe\r\n\r\nReboot the IPMI card:\r\n\r\n    ./jockey ipmi tool 10.0.1.1 mc reset cold\r\n\r\nGet sensor output:\r\n\r\n    ./jockey ipmi tool 10.0.1.1 sdr list\r\n\r\nGet the error log:\r\n\r\n    ./jockey ipmi tool 10.0.1.1 sel elist\r\n\r\n### The Parallel Genders Trick\r\n\r\n## InfiniBand Basics\r\nInfiniBand is a switched fabric communications link used in high-performance computing and enterprise data centers. If you need RDMA you need InfiniBand. You have to run the subnet manager (OpenSM) which assigns Local IDentifiers (LIDs) to each port connected to the InfiniBand fabric, and develops a routing table based off of the assigned LIDs.There are two types of SMs, software based and hardware based. Hardware based subnet managers are typically part of the firmware of the attached InfiniBand switch. Buy a switch with HW-based SM.\r\n\r\n\r\n## Ansible Bootstrap\r\nInstall ansible on the host. Ansible should be installed into `$HOME/ansible`:\r\n\r\n    cd $HOME\r\n    git clone git://github.com/ansible/ansible.git\r\n\r\nEdit your `$HOME/.bashrc`:\r\n\r\n    source $HOME/ansible/hacking/env-setup &> /dev/null\r\n\r\nRun the source command:\r\n\r\n    source $HOME/.bashrc\r\n\r\nAnsible is used to further provision root servers on the stage 2 level. Stage 2 is responsible to reach the production ready state of the grid.\r\n\r\nFrom now on all commands are relative to `$HOME/gridhowto`:\r\n\r\n    cd $HOME/gridhowto\r\n\r\nEdit `hosts` file:\r\n\r\n    [root]\r\n    root-01 ansible_ssh_host=10.1.1.1\r\n    root-02 ansible_ssh_host=10.1.1.2\r\n\r\nCheck the connection:\r\n\r\n    bin/ping root@root-01\r\n\r\nThe bootstrap playbook creates the admin wheel user. You have to bootstrap each machine separately since root passwords are different:\r\n\r\n    ssh-keygen -f keys/admin\r\n    pushd keys; ln -s admin root; popd\r\n    bin/play root@root-01 bootstrap\r\n    bin/play root@root-02 bootstrap\r\n    ...\r\n\r\nThe following operator shortcuts are used: `@` is `-k` and `@@` is `-k --sudo`.\r\n\r\nTest the bootstrap:\r\n\r\n    bin/ping admin@root\r\n\r\nBy securing the server you lock out root. Only admin is allowed to login with keys thereafter:\r\n\r\n    bin/play @@root secure\r\n\r\nReboot or shutdown the machines by:\r\n\r\n    bin/reboot @@root\r\n    bin/shutdown @@root\r\n\r\nCreate a new LVM partition:\r\n\r\n    bin/admin root run \"lvcreate -l 30%FREE -n data vg_root\" -k --sudo\r\n\r\n\r\n## Basic Services\r\nRoot servers provide NTP for the cluster. If you have a very large cluster root servers talk only to satellite servers aka rack leaders. Root servers are stratum 2 time servers. Each root server broadcasts time to the system network with crypto enabled.\r\n\r\nBasic services contain NTP, Rsyslog and DNSmasq hosts cache:\r\n\r\n    bin/play @@root basic\r\n\r\nRoot server names are cached in `/etc/hosts.d/root`. Put DNS cache files (hosts) in `/etc/hosts.d` and notify dnsmasq to reload. DHCP client overwrites `resolv.conf` so you have to set an interface specific conf in `etc/dhcp` if you use DHCP. Rsyslog does cross-logging between root servers.\r\n\r\n## Ganglia\r\nGanglia is a scalable distributed monitoring system for high-performance computing systems such as clusters and Grids. It is based on a hierarchical design targeted at federations of clusters. You can think of it as a low-level cluster top. Ganglia is running with unicast addresses and root servers cross-monitor each other.\r\n\r\n    bin/play @@root ganglia\r\n\r\nGanglia will install haproxy\r\n\r\n### Firewall\r\nEnable basic Shorewall firewall on the root servers:\r\n\r\n    bin/play @@root shorewall\r\n\r\nNote that emergency rules are defined in `etc/shorewall/rulestopped.j2`. Enable SSH on the provision interfaces. \r\n\r\n#### Ferm\r\n\r\n\r\n## Tinc VPN\r\n\r\n### Root Server VPN\r\n\r\n### Tinc Over TOR\r\n\r\n## Cluster FS 1\r\n\r\n### Glusterfs\r\n\r\n### DRBD\r\n\r\n## HA Mysql\r\n\r\n## HA Slurm\r\n\r\n## HA XCat\r\n\r\n## Grid\r\n### Globus\r\n#### PKI\r\n#### GSI-SSH\r\n#### GridFTP\r\n### GateONE","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}